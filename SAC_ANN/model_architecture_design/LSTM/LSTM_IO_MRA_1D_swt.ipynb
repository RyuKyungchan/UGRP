{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWT-MRA(1D)를 input으로 넣고 1D를 output으로 출력하는 모델 -> MRA level을 feature로 봄\n",
    "#### Contaminated, Clean Scale을 다르게 함. (각자 scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import pywt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../tool_code/python_tool_code/function/') # \"~~/tool_code/plot/\" (상대 경로)\n",
    "from DataPlot import Data_Load_Plot, Result_Plot, Loss_Plot\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 load & plot\n",
    "datapath=\"C:/Users/User/Desktop/SCH/paper_data/\"\n",
    "\n",
    "Contaminated_data, Clean_data, Artifact_data = Data_Load_Plot(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWT 수행 (MRA)\n",
    "wavelet = 'db1'\n",
    "mra_sig = pywt.mra(Clean_data[0], wavelet, transform='swt')\n",
    "mra_art = pywt.mra(Contaminated_data[0], wavelet, transform='swt')\n",
    "\n",
    "mra_sig = np.array(mra_sig)\n",
    "mra_art = np.array(mra_art)\n",
    "\n",
    "vmin, vmax = min(mra_sig.min(), mra_art.min()), max(mra_sig.max(), mra_art.max())\n",
    "\n",
    "t = np.linspace(0, 2, num=4000)\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mra_sig, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax, extent=[t[0], t[-1], 11, 0])\n",
    "plt.colorbar().set_label('Coefficient Value', fontsize=15)\n",
    "plt.xlabel('time(s)', fontsize=15)\n",
    "plt.ylabel('level', fontsize=15)\n",
    "plt.title('Clean Signal', fontsize=18)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mra_art, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax, extent=[t[0], t[-1], 11, 0])\n",
    "plt.colorbar().set_label('Coefficient Value', fontsize=15)\n",
    "plt.xlabel('time(s)', fontsize=15)\n",
    "plt.ylabel('level', fontsize=15)\n",
    "plt.title('Signal with Artifact', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wt = []\n",
    "y_wt = []\n",
    "\n",
    "wavelet = 'db1'\n",
    "\n",
    "for x, y in zip(Contaminated_data, Clean_data):\n",
    "    mra_x = pywt.mra(x, wavelet, transform='swt')\n",
    "    mra_y = pywt.mra(y, wavelet, transform='swt')\n",
    "    X_wt.append(mra_x)\n",
    "    y_wt.append(mra_y)\n",
    "\n",
    "X_wt = np.array(X_wt)\n",
    "y_wt = np.array(y_wt)\n",
    "\n",
    "print(\"X_wt:\", X_wt.shape)\n",
    "print(\"y_wt:\", y_wt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRA 결과 scaling\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X = []\n",
    "y = [] \n",
    "\n",
    "scaler_x.fit(X_wt[0].flatten().reshape(-1, 1))\n",
    "scaler_y.fit(y_wt[0].flatten().reshape(-1, 1))\n",
    "\n",
    "for xx, yy in zip(X_wt, y_wt):\n",
    "    flat_x = xx.flatten().reshape(-1, 1)\n",
    "    flat_y = yy.flatten().reshape(-1, 1)\n",
    "    scaled_flat_x = scaler_x.transform(flat_x)\n",
    "    scaled_flat_y = scaler_y.transform(flat_y) # X, y 각자 scaling\n",
    "    X.append(scaled_flat_x.reshape(xx.shape))\n",
    "    y.append(scaled_flat_y.reshape(yy.shape))\n",
    "\n",
    "X = np.transpose(np.array(X), (0, 2, 1))\n",
    "y = np.transpose(np.array(y), (0, 2, 1))\n",
    "\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"<Data Shape>\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_train shape: {X_train.shape}\\ny_train shape: {y_train.shape}\") # x : B x T, y : B x T\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_test shape: {X_test.shape}\\ny_test shape: {y_test.shape}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Define a fully connected layer that outputs the same size as the input features\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) # Initialize hidden state with zeros\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) # Initialize cell state with zeros\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode the hidden state of all time steps\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_size = 6  # Number of features\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 6  # Same as the number of input features for regression\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "num_epochs = 500\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # model.eval()\n",
    "    # test_loss = 0.0\n",
    "    # with torch.no_grad():\n",
    "    #     for x, y in test_loader:\n",
    "    #         x, y = x.to(device), y.to(device)\n",
    "    #         y_pred = model(x)\n",
    "    #         loss = criterion(y_pred, y)\n",
    "    #         test_loss += loss.item() * x.size(0)\n",
    "    # test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_list.append(epoch_loss)\n",
    "    # test_list.append(test_loss)\n",
    "    if (epoch+1)%1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_Plot(train_list)\n",
    "# Loss_Plot(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 저장\n",
    "torch.save(model.state_dict(), '../model_saved/LSTM_IO_MRA_1D_swt_10000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "Contaminated = torch.tensor([])\n",
    "Clean = torch.tensor([])\n",
    "SACed = torch.tensor([])\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        test_loss += loss.item() * x.size(0)\n",
    "\n",
    "        Contaminated = torch.cat((Contaminated, x.permute(0, 2, 1).cpu()), 0)\n",
    "        SACed = torch.cat((SACed, y_pred.permute(0, 2, 1).cpu()), 0)\n",
    "        Clean = torch.cat((Clean, y.permute(0, 2, 1).cpu()), 0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRA Plot\n",
    "SACed_ex = SACed[0].detach().cpu()\n",
    "Clean_ex = Clean[0].detach().cpu()\n",
    "\n",
    "vmin, vmax = min(SACed_ex.min(), Clean_ex.min()), max(SACed_ex.max(), Clean_ex.max())\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(SACed_ex, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax, extent=[t[0], t[-1], 11, 0])\n",
    "plt.colorbar().set_label('Coefficient Value', fontsize=15)\n",
    "plt.xlabel('time(s)', fontsize=15)\n",
    "plt.ylabel('level', fontsize=15)\n",
    "plt.title('SACed signal', fontsize=20)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(Clean_ex, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax, extent=[t[0], t[-1], 11, 0])\n",
    "plt.colorbar().set_label('Coefficient Value', fontsize=15)\n",
    "plt.xlabel('time(s)', fontsize=15)\n",
    "plt.ylabel('level', fontsize=15)\n",
    "plt.title('Clean signal', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(Clean_ex, SACed_ex)}\")\n",
    "print(f\"Mean Squared Error: {mean_squared_error(Clean_ex, SACed_ex)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRA 결과 inverse scaling\n",
    "Contaminated_inverse_scaled = []\n",
    "SACed_inverse_scaled = []\n",
    "Clean_inverse_scaled = []\n",
    "\n",
    "for xx, yhat, yy in zip(Contaminated, SACed, Clean):\n",
    "    flat_x = xx.flatten().reshape(-1, 1)\n",
    "    flat_yhat = yhat.flatten().reshape(-1, 1)\n",
    "    flat_y = yy.flatten().reshape(-1, 1)\n",
    "\n",
    "    inv_flat_x = scaler_x.inverse_transform(flat_x)\n",
    "    inv_flat_yhat = scaler_y.inverse_transform(flat_yhat)\n",
    "    inv_flat_y = scaler_y.inverse_transform(flat_y)\n",
    "\n",
    "    Contaminated_inverse_scaled.append(inv_flat_x.reshape(xx.shape))\n",
    "    SACed_inverse_scaled.append(inv_flat_yhat.reshape(yhat.shape))\n",
    "    Clean_inverse_scaled.append(inv_flat_y.reshape(yy.shape))\n",
    "\n",
    "Contaminated_inverse_scaled = np.array(Contaminated_inverse_scaled)\n",
    "SACed_inverse_scaled = np.array(SACed_inverse_scaled)\n",
    "Clean_inverse_scaled = np.array(Clean_inverse_scaled)\n",
    "\n",
    "\n",
    "# MRA 데이터를 inverse하여 시계열 데이터로 변환\n",
    "\n",
    "Contaminated_invdwt = []\n",
    "SACed_invdwt = []\n",
    "Clean_invdwt = []\n",
    "\n",
    "for mra_x, mra_yhat, mra_y in zip(Contaminated_inverse_scaled, SACed_inverse_scaled, Clean_inverse_scaled):\n",
    "    contaminated_invdwt = pywt.imra(mra_x)\n",
    "    saced_invdwt = pywt.imra(mra_yhat)\n",
    "    clean_invdwt = pywt.imra(mra_y)\n",
    "    \n",
    "    Contaminated_invdwt.append(contaminated_invdwt)\n",
    "    SACed_invdwt.append(saced_invdwt)\n",
    "    Clean_invdwt.append(clean_invdwt)\n",
    "\n",
    "Contaminated_invdft = np.array(Contaminated_invdwt)\n",
    "SACed_invdft = np.array(SACed_invdwt)\n",
    "Clean_invdft = np.array(Clean_invdwt)\n",
    "\n",
    "print(\"X_reconstructed:\", Contaminated_invdft.shape)\n",
    "print(\"yhat_reconstructed:\", SACed_invdft.shape)\n",
    "print(\"y_reconstructed:\", Clean_invdft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 Plot\n",
    "from DataPlot import Data_Load_Plot, Result_Plot_paper, Loss_Plot\n",
    "save_path = '../../../result/data_10000/paper/'\n",
    "save_title = 'LSTM_IO_MRA_1D_swt'\n",
    "Result_Plot(Contaminated_invdft, SACed_invdft, Clean_invdft) # inverse scaled data를 input으로 넣음\n",
    "# Result_Plot_paper(Contaminated_invdft, SACed_invdft, Clean_invdft, save_path, save_title) # inverse scaled data를 input으로 넣음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
