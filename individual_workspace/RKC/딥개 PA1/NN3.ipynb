{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class linear():\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weight = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros(output_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return np.dot(input, self.weight) + self.bias\n",
    "    \n",
    "    def backward(self, input, output_gradient, learning_rate): # Backpropagation\n",
    "        # input: 뉴런에 들어가는 값\n",
    "        # output_gradient: 다음 뉴런의 기울기\n",
    "        input_gradient = np.dot(output_gradient, self.weight.T) # 다음 layer에 넘겨줄 gradient\n",
    "        weight_gradient = np.dot(input.T, output_gradient) # 업데이트 할 weight gradient\n",
    "        bias_gradient = np.sum(output_gradient, axis=0)\n",
    "        \n",
    "        self.weight -= learning_rate*weight_gradient\n",
    "        self.bias -= learning_rate*bias_gradient\n",
    "        \n",
    "        return input_gradient\n",
    "    \n",
    "class ReLU():\n",
    "    def forward(self, input):\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def backward(self, input, output_gradient):\n",
    "        grad = input > 0\n",
    "        return grad * output_gradient   \n",
    "              \n",
    "def SoftMax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def CrossEntropyLoss(y, y_pred):\n",
    "    c = 1e-10\n",
    "    y_pred = np.clip(y_pred, c ,1-c)\n",
    "    return -np.sum(y * np.log(y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_3_layer():\n",
    "    def __init__(self, in_feature, hid_feature, out_feature):\n",
    "        self.inputs = [[] for i in range(6)]\n",
    "        self.layer1 = linear(in_feature, hid_feature)\n",
    "        self.relu1 = ReLU()\n",
    "        self.layer2 = linear(hid_feature, hid_feature)\n",
    "        self.relu2 = ReLU()\n",
    "        self.layer3 = linear(hid_feature, out_feature)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.inputs[0] = input\n",
    "        self.inputs[1] = self.layer1.forward(self.inputs[0])\n",
    "        self.inputs[2] = self.relu1.forward(self.inputs[1])\n",
    "        self.inputs[3] = self.layer2.forward(self.inputs[2])\n",
    "        self.inputs[4] = self.relu2.forward(self.inputs[3])\n",
    "        self.inputs[5] = self.layer3.forward(self.inputs[4])\n",
    "        output = SoftMax(self.inputs[5])\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        inputs = self.inputs\n",
    "        output_gradient = self.layer3.backward(inputs[4], output_gradient, learning_rate)\n",
    "        output_gradient = self.relu2.backward(inputs[3], output_gradient)\n",
    "        output_gradient = self.layer2.backward(inputs[2], output_gradient, learning_rate)\n",
    "        output_gradient = self.relu1.backward(inputs[1], output_gradient)\n",
    "        output_gradient = self.layer1.backward(inputs[0], output_gradient, learning_rate)\n",
    "        return output_gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\tLoss: 3.0578567105613046\n",
      "Epoch [2/100]\tLoss: 0.892939724518357\n",
      "Epoch [3/100]\tLoss: 0.6164063806685609\n",
      "Epoch [4/100]\tLoss: 0.4621200976469724\n",
      "Epoch [5/100]\tLoss: 0.36740002706878244\n",
      "Epoch [6/100]\tLoss: 0.31988543352598375\n",
      "Epoch [7/100]\tLoss: 0.27801067187031087\n",
      "Epoch [8/100]\tLoss: 0.2631394386042169\n",
      "Epoch [9/100]\tLoss: 0.20421018803284902\n",
      "Epoch [10/100]\tLoss: 0.19491995933598125\n",
      "Epoch [11/100]\tLoss: 0.17119999999588448\n",
      "Epoch [12/100]\tLoss: 0.15935532127613658\n",
      "Epoch [13/100]\tLoss: 0.17583717329061396\n",
      "Epoch [14/100]\tLoss: 0.1445127002703047\n",
      "Epoch [15/100]\tLoss: 0.1363498218965097\n",
      "Epoch [16/100]\tLoss: 0.10470804875639192\n",
      "Epoch [17/100]\tLoss: 0.13752911704083\n",
      "Epoch [18/100]\tLoss: 0.11902833240731024\n",
      "Epoch [19/100]\tLoss: 0.1836079852496001\n",
      "Epoch [20/100]\tLoss: 0.1079838285470133\n",
      "Epoch [21/100]\tLoss: 0.12801135809228542\n",
      "Epoch [22/100]\tLoss: 0.0986516707552883\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m CrossEntropyLoss(labels, output)\n\u001b[0;32m     24\u001b[0m     output_gradient \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m-\u001b[39m labels\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_gradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m train_loss_list\u001b[38;5;241m.\u001b[39mappend(running_train_loss \u001b[38;5;241m/\u001b[39m train_loader\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m())\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_train_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36mNN_3_layer.backward\u001b[1;34m(self, output_gradient, learning_rate)\u001b[0m\n\u001b[0;32m     24\u001b[0m output_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2\u001b[38;5;241m.\u001b[39mbackward(inputs[\u001b[38;5;241m2\u001b[39m], output_gradient, learning_rate)\n\u001b[0;32m     25\u001b[0m output_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1\u001b[38;5;241m.\u001b[39mbackward(inputs[\u001b[38;5;241m1\u001b[39m], output_gradient)\n\u001b[1;32m---> 26\u001b[0m output_gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_gradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_gradient\n",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m, in \u001b[0;36mlinear.backward\u001b[1;34m(self, input, output_gradient, learning_rate)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, output_gradient, learning_rate): \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# input: 뉴런에 들어가는 값\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# output_gradient: 다음 뉴런의 기울기\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     input_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(output_gradient, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;66;03m# 다음 layer에 넘겨줄 gradient\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     weight_gradient \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_gradient\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 업데이트 할 weight gradient\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     bias_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(output_gradient, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate\u001b[38;5;241m*\u001b[39mweight_gradient\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dataset.dataloader import Dataloader\n",
    "\n",
    "model = NN_3_layer(in_feature=784, hid_feature=128, out_feature=10)\n",
    "\n",
    "train_loader = Dataloader('dataset/', is_train=True, batch_size=8, shuffle=True)\n",
    "test_loader = Dataloader('dataset/', is_train=False, batch_size=8, shuffle=False)\n",
    "\n",
    "input_data = np.random.randn(28, 28)\n",
    "input_data = input_data.reshape(1, 784)\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    running_train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        input_data = images.reshape(images.shape[0], -1)\n",
    "        output = model.forward(input_data)\n",
    "        running_train_loss += CrossEntropyLoss(labels, output)\n",
    "               \n",
    "        output_gradient = output - labels\n",
    "        model.backward(output_gradient, learning_rate)\n",
    "        \n",
    "    train_loss_list.append(running_train_loss / train_loader.__len__())\n",
    "    print(f\"Epoch [{i+1}/{epochs}]\\tLoss: {running_train_loss / train_loader.__len__()}\")\n",
    "    \n",
    "    running_test_loss = 0\n",
    "    for images, labels in test_loader:\n",
    "        input_data = images.reshape(images.shape[0], -1)\n",
    "        output = model.forward(input_data)\n",
    "        running_test_loss += CrossEntropyLoss(labels, output)\n",
    "        \n",
    "    test_loss_list.append(running_test_loss / test_loader.__len__())\n",
    "    \n",
    "\n",
    "plt.plot(train_loss_list, label='Train')\n",
    "plt.plot(test_loss_list, label='Test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train / Test Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in test_loader:\n",
    "    images = images.reshape(images.shape[0], -1)  # (배치 크기, 784)\n",
    "    outputs = model.forward(images)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
