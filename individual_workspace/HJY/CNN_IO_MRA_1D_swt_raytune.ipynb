{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWT-MRA(1D)를 input으로 넣고 1D를 output으로 출력하는 모델 -> MRA level을 feature로 봄\n",
    "#### Contaminated, Clean Scale을 다르게 함. (각자 scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 22:07:39,980\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "# Ray Tune library\n",
    "\n",
    "from functools import partial\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init(local_mode=True)\n",
    "\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import pywt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../tool_code/python_tool_code/function/') # \"~~/tool_code/plot/\" (상대 경로)\n",
    "from DataPlot import Data_Load_Plot, Result_Plot, Loss_Plot, Result_Plot_paper\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=  \"C:/Users/stell/OneDrive/바탕 화면/10000_data/\"\n",
    "\n",
    "def load_data(data_dir = datapath):\n",
    "    Contaminated_data, Clean_data, Artifact_data = Data_Load_Plot(datapath)\n",
    "\n",
    "    Contaminated_data = Contaminated_data[0:100]\n",
    "    Clean_data = Clean_data[0:100]\n",
    "    Artifact_data = Artifact_data[0:100]\n",
    "\n",
    "    X_wt = []\n",
    "    y_wt = []\n",
    "\n",
    "    wavelet = 'db1'\n",
    "\n",
    "    for x, y in zip(Contaminated_data, Clean_data):\n",
    "        mra_x = pywt.mra(x, wavelet, transform='swt')\n",
    "        mra_y = pywt.mra(y, wavelet, transform='swt')\n",
    "        X_wt.append(mra_x)\n",
    "        y_wt.append(mra_y)\n",
    "\n",
    "    X_wt = np.array(X_wt)\n",
    "    y_wt = np.array(y_wt)\n",
    "\n",
    "    print(\"X_wt:\", X_wt.shape)\n",
    "    print(\"y_wt:\", y_wt.shape)\n",
    "\n",
    "    # MRA 결과 scaling\n",
    "    scaler_x = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X = []\n",
    "    y = [] \n",
    "\n",
    "    scaler_x.fit(X_wt[0].flatten().reshape(-1, 1))\n",
    "    scaler_y.fit(y_wt[0].flatten().reshape(-1, 1))\n",
    "\n",
    "    for xx, yy in zip(X_wt, y_wt):\n",
    "        flat_x = xx.flatten().reshape(-1, 1)\n",
    "        flat_y = yy.flatten().reshape(-1, 1)\n",
    "        scaled_flat_x = scaler_x.transform(flat_x)\n",
    "        scaled_flat_y = scaler_y.transform(flat_y) # X, y 각자 scaling\n",
    "        X.append(scaled_flat_x.reshape(xx.shape))\n",
    "        y.append(scaled_flat_y.reshape(yy.shape))\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contaminated_data.shape: (10000, 4000)\n",
      "Clean_data.shape: (10000, 4000)\n",
      "Figure(1200x500)\n",
      "Figure(1500x700)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 load & plot\n",
    "datapath=  \"C:/Users/stell/OneDrive/바탕 화면/10000_data/\"\n",
    "\n",
    "Contaminated_data, Clean_data, Artifact_data = Data_Load_Plot(datapath)\n",
    "\n",
    "Contaminated_data = Contaminated_data[0:100]\n",
    "Clean_data = Clean_data[0:100]\n",
    "Artifact_data = Artifact_data[0:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_wt: (100, 6, 4000)\n",
      "y_wt: (100, 6, 4000)\n"
     ]
    }
   ],
   "source": [
    "X_wt = []\n",
    "y_wt = []\n",
    "\n",
    "wavelet = 'db1'\n",
    "\n",
    "for x, y in zip(Contaminated_data, Clean_data):\n",
    "    mra_x = pywt.mra(x, wavelet, transform='swt')\n",
    "    mra_y = pywt.mra(y, wavelet, transform='swt')\n",
    "    X_wt.append(mra_x)\n",
    "    y_wt.append(mra_y)\n",
    "\n",
    "X_wt = np.array(X_wt)\n",
    "y_wt = np.array(y_wt)\n",
    "\n",
    "print(\"X_wt:\", X_wt.shape)\n",
    "print(\"y_wt:\", y_wt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (100, 6, 4000)\n",
      "y: (100, 6, 4000)\n"
     ]
    }
   ],
   "source": [
    "# MRA 결과 scaling\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X = []\n",
    "y = [] \n",
    "\n",
    "scaler_x.fit(X_wt[0].flatten().reshape(-1, 1))\n",
    "scaler_y.fit(y_wt[0].flatten().reshape(-1, 1))\n",
    "\n",
    "for xx, yy in zip(X_wt, y_wt):\n",
    "    flat_x = xx.flatten().reshape(-1, 1)\n",
    "    flat_y = yy.flatten().reshape(-1, 1)\n",
    "    scaled_flat_x = scaler_x.transform(flat_x)\n",
    "    scaled_flat_y = scaler_y.transform(flat_y) # X, y 각자 scaling\n",
    "    X.append(scaled_flat_x.reshape(xx.shape))\n",
    "    y.append(scaled_flat_y.reshape(yy.shape))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Data Shape>\n",
      "-----------------------------\n",
      "X_train shape: (80, 6, 4000)\n",
      "y_train shape: (80, 6, 4000)\n",
      "-----------------------------\n",
      "X_test shape: (20, 6, 4000)\n",
      "y_test shape: (20, 6, 4000)\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"<Data Shape>\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_train shape: {X_train.shape}\\ny_train shape: {y_train.shape}\") # x : B x T, y : B x T\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_test shape: {X_test.shape}\\ny_test shape: {y_test.shape}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test data 저장\n",
    "\n",
    "# # MRA 결과 inverse scaling\n",
    "# X_test_MRAinv = []\n",
    "# y_test_MRAinv = []\n",
    "\n",
    "# for xx, yy in zip(X_test, y_test):\n",
    "#     flat_x = xx.flatten().reshape(-1, 1)\n",
    "#     flat_y = yy.flatten().reshape(-1, 1)\n",
    "\n",
    "#     inv_flat_x = scaler_x.inverse_transform(flat_x)\n",
    "#     inv_flat_y = scaler_y.inverse_transform(flat_y)\n",
    "\n",
    "#     X_test_MRAinv.append(inv_flat_x.reshape(xx.shape))\n",
    "#     y_test_MRAinv.append(inv_flat_y.reshape(yy.shape))\n",
    "\n",
    "# X_test_MRAinv = np.array(X_test_MRAinv)\n",
    "# y_test_MRAinv = np.array(y_test_MRAinv)\n",
    "\n",
    "\n",
    "# # MRA 데이터를 inverse하여 시계열 데이터로 변환\n",
    "# X_test_time = []\n",
    "# y_test_time = []\n",
    "\n",
    "# for mra_x, mra_y in zip(X_test_MRAinv, y_test_MRAinv):\n",
    "#     time_X = pywt.imra(mra_x)\n",
    "#     time_y = pywt.imra(mra_y)\n",
    "    \n",
    "#     X_test_time.append(time_X)\n",
    "#     y_test_time.append(time_y)\n",
    "\n",
    "# X_test_time = np.array(X_test_time)\n",
    "# y_test_time = np.array(y_test_time)\n",
    "\n",
    "# print(\"X_reconstructed:\", X_test_time.shape)\n",
    "# print(\"y_reconstructed:\", y_test_time.shape)\n",
    "\n",
    "# np.save(\"../../../evaluation/Test_data/contaminated_by_realistic.npy\", X_test_time)\n",
    "# np.save(\"../../../evaluation/Test_data/clean_data.npy\", y_test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_MRA_1D(nn.Module):\n",
    "    def __init__(self, l1=512, in_channels=6, out_channels=6):\n",
    "        super(CNN_MRA_1D, self).__init__()\n",
    "        \n",
    "        # Encoding path\n",
    "        self.conv1 = nn.Conv1d(in_channels, l1//8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(l1//8, l1//4, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(l1//4, l1//2, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(l1//2, l1, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        \n",
    "        # Decoding path\n",
    "        self.upconv4 = nn.ConvTranspose1d(l1, l1/2, kernel_size=2, stride=2)\n",
    "        self.upconv3 = nn.ConvTranspose1d(l1/2, l1/4, kernel_size=2, stride=2)\n",
    "        self.upconv2 = nn.ConvTranspose1d(l1/4, l1/8, kernel_size=2, stride=2)\n",
    "        self.upconv1 = nn.Conv1d(l1/8, out_channels, kernel_size=1)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x1 = self.gelu(self.conv1(x))\n",
    "        x2 = self.pool(self.gelu(self.conv2(x1)))\n",
    "        x3 = self.pool(self.gelu(self.conv3(x2)))\n",
    "        x4 = self.pool(self.gelu(self.conv4(x3)))\n",
    "\n",
    "        # Decoding\n",
    "        x4_up = self.upconv4(x4)\n",
    "        x3_up = self.upconv3(x4_up)\n",
    "        x2_up = self.upconv2(x3_up)\n",
    "        x_out = self.upconv1(x2_up)\n",
    "        \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Tune 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ray(config, data_dir=None):\n",
    "    model = CNN_MRA_1D(config[\"l1\"], in_channels=6, out_channels=6).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])    \n",
    "\n",
    "    # Check Point 저장\n",
    "    checkpoint = get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                checkpoint_state = pickle.load(fp)\n",
    "            start_epoch = checkpoint_state[\"epoch\"]\n",
    "            model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    trainset, testset = train_dataset, test_dataset\n",
    "\n",
    "    test_abs = int(len(trainset) * 0.8)\n",
    "    train_subset, val_subset = random_split(\n",
    "        trainset, [test_abs, len(trainset) - test_abs]\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
    "    )\n",
    "\n",
    "    for epoch in range(start_epoch, 10):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0): # 0 -> index가 0부터 시작\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, i + 1, running_loss / epoch_steps)\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "        \n",
    "        # Ray Tune으로 통신하기\n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(data_path, \"wb\") as fp:\n",
    "                pickle.dump(checkpoint_data, fp)\n",
    "\n",
    "            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "            train.report(\n",
    "                {\"loss\": val_loss / val_steps},\n",
    "                checkpoint=checkpoint,\n",
    "            )\n",
    "        \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, device=\"cpu\"):\n",
    "    trainset, testset = train_dataset, test_dataset\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=32, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검색 공간 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contaminated_data.shape: (10000, 4000)\n",
      "Clean_data.shape: (10000, 4000)\n",
      "Figure(1200x500)\n",
      "Figure(1500x700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 22:02:56,739\tINFO tune.py:613 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_wt: (100, 6, 4000)\n",
      "y_wt: (100, 6, 4000)\n",
      "pass1\n",
      "pass2\n",
      "<IPython.core.display.HTML object>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      ":task_name:bundle_reservation_check_func\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":task_name:bundle_reservation_check_func\n",
      ":actor_name:ImplicitFunc\n",
      ":actor_name:func\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":actor_name:ImplicitFunc\n",
      ":actor_name:func\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/stell/AppData/Local/Temp/ray/session_2024-09-11_21-58-26_140596_26880/artifacts/2024-09-11_22-02-56/train_ray_2024-09-11_22-02-56/driver_artifacts/train_ray_2a16c_00000_0_batch_size=2,l1=512,lr=0.0001_2024-09-11_22-02-57\\\\events.out.tfevents.1726059777.DESKTOP-T9EUN1U'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\tensorboardX\\record_writer.py:58\u001b[0m, in \u001b[0;36mopen_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     57\u001b[0m prefix \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 58\u001b[0m factory \u001b[38;5;241m=\u001b[39m \u001b[43mREGISTERED_FACTORIES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m factory\u001b[38;5;241m.\u001b[39mopen(path)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'C'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26880\\3510506156.py:62\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial test set accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_acc))\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# 매 실험당 사용할 GPU 수를 여기에서 변경할 수 있습니다:\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26880\\3510506156.py:23\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[0;32m     13\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ASHAScheduler(\n\u001b[0;32m     14\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     reduction_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatapath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus_per_trial\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#local_dir=\"./ray_results\"  # 체크포인트가 저장될 디렉토리 경로\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mget_best_trial(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\ray\\tune\\tune.py:1001\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mis_finished() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m-> 1001\u001b[0m         \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m has_verbosity(Verbosity\u001b[38;5;241m.\u001b[39mV1_EXPERIMENT):\n\u001b[0;32m   1003\u001b[0m             _report_progress(runner, progress_reporter)\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:686\u001b[0m, in \u001b[0;36mTuneController.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_add_actors()\n\u001b[0;32m    685\u001b[0m \u001b[38;5;66;03m# Handle one event\u001b[39;00m\n\u001b[1;32m--> 686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;66;03m# If there are no actors running, warn about potentially\u001b[39;00m\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;66;03m# insufficient resources\u001b[39;00m\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_manager\u001b[38;5;241m.\u001b[39mnum_live_actors:\n\u001b[0;32m    690\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_resources_manager\u001b[38;5;241m.\u001b[39mon_no_available_trials(\n\u001b[0;32m    691\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_trials()\n\u001b[0;32m    692\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\ray\\air\\execution\\_internal\\actor_manager.py:222\u001b[0m, in \u001b[0;36mRayActorManager.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    219\u001b[0m [future] \u001b[38;5;241m=\u001b[39m ready\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m actor_state_futures:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor_state_events\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve_future\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m actor_task_futures:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_task_events\u001b[38;5;241m.\u001b[39mresolve_future(future)\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py:118\u001b[0m, in \u001b[0;36mRayEventManager.resolve_future\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_result:\n\u001b[1;32m--> 118\u001b[0m         \u001b[43mon_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\ray\\air\\execution\\_internal\\actor_manager.py:381\u001b[0m, in \u001b[0;36mRayActorManager._try_start_actors.<locals>.create_callbacks.<locals>.on_actor_start\u001b[1;34m(result)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_actor_start\u001b[39m(result: Any):\n\u001b[1;32m--> 381\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor_start_resolved\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracked_actor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracked_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuture\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\ray\\air\\execution\\_internal\\actor_manager.py:243\u001b[0m, in \u001b[0;36mRayActorManager._actor_start_resolved\u001b[1;34m(self, tracked_actor, future)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracked_actors_to_state_futures[tracked_actor]\u001b[38;5;241m.\u001b[39mremove(future)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracked_actor\u001b[38;5;241m.\u001b[39m_on_start:\n\u001b[1;32m--> 243\u001b[0m     \u001b[43mtracked_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracked_actor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:1132\u001b[0m, in \u001b[0;36mTuneController._actor_started\u001b[1;34m(self, tracked_actor, log)\u001b[0m\n\u001b[0;32m   1127\u001b[0m ray_actor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_manager\u001b[38;5;241m.\u001b[39m_live_actors_to_ray_actors_resources[\n\u001b[0;32m   1128\u001b[0m     tracked_actor\n\u001b[0;32m   1129\u001b[0m ][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1130\u001b[0m trial\u001b[38;5;241m.\u001b[39mset_ray_actor(ray_actor)\n\u001b[1;32m-> 1132\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_start\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trial_status(trial, Trial\u001b[38;5;241m.\u001b[39mRUNNING)\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_trial_to_checkpoint(trial)\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\ray\\tune\\callback.py:398\u001b[0m, in \u001b[0;36mCallbackList.on_trial_start\u001b[1;34m(self, **info)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_start\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minfo):\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callbacks:\n\u001b[1;32m--> 398\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_start\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\ray\\tune\\logger\\logger.py:147\u001b[0m, in \u001b[0;36mLoggerCallback.on_trial_start\u001b[1;34m(self, iteration, trials, trial, **info)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_start\u001b[39m(\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m, iteration: \u001b[38;5;28mint\u001b[39m, trials: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial\u001b[39m\u001b[38;5;124m\"\u001b[39m], trial: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrial\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minfo\n\u001b[0;32m    146\u001b[0m ):\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_trial_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\ray\\tune\\logger\\tensorboardx.py:205\u001b[0m, in \u001b[0;36mTBXLoggerCallback.log_trial_start\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_writer[trial]\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    204\u001b[0m trial\u001b[38;5;241m.\u001b[39minit_local_path()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_writer[trial] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_summary_writer_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush_secs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[0;32m    207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_result[trial] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\tensorboardX\\writer.py:300\u001b[0m, in \u001b[0;36mSummaryWriter.__init__\u001b[1;34m(self, logdir, comment, purge_step, max_queue, flush_secs, filename_suffix, write_to_disk, log_dir, comet_config, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Initialize the file writers, but they can be cleared out on close\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# and recreated later as needed.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_writers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_file_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# Create default bins for histograms, see generate_testdata.py in tensorflow/tensorboard\u001b[39;00m\n\u001b[0;32m    303\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1E-12\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\tensorboardX\\writer.py:348\u001b[0m, in \u001b[0;36mSummaryWriter._get_file_writer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_writer\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_writers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_writer \u001b[38;5;241m=\u001b[39m \u001b[43mFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmax_queue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_queue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mflush_secs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush_secs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mfilename_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filename_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpurge_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_writer\u001b[38;5;241m.\u001b[39madd_event(\n\u001b[0;32m    355\u001b[0m             Event(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpurge_step, file_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrain.Event:2\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\tensorboardX\\writer.py:104\u001b[0m, in \u001b[0;36mFileWriter.__init__\u001b[1;34m(self, logdir, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Sometimes PosixPath is passed in and we need to coerce it to\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# a string in all cases\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# TODO: See if we can remove this in the future if we are\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# actually the ones passing in a PosixPath\u001b[39;00m\n\u001b[0;32m    103\u001b[0m logdir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(logdir)\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_writer \u001b[38;5;241m=\u001b[39m \u001b[43mEventFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush_secs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_suffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcleanup\u001b[39m():\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\tensorboardX\\event_file_writer.py:106\u001b[0m, in \u001b[0;36mEventFileWriter.__init__\u001b[1;34m(self, logdir, max_queue_size, flush_secs, filename_suffix)\u001b[0m\n\u001b[0;32m    104\u001b[0m directory_check(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logdir)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_queue \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mQueue(max_queue_size)\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ev_writer \u001b[38;5;241m=\u001b[39m \u001b[43mEventsWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_suffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush_secs \u001b[38;5;241m=\u001b[39m flush_secs\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\tensorboardX\\event_file_writer.py:43\u001b[0m, in \u001b[0;36mEventsWriter.__init__\u001b[1;34m(self, file_prefix, filename_suffix)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_name \u001b[38;5;241m=\u001b[39m file_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.out.tfevents.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())[:\u001b[38;5;241m10\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m     41\u001b[0m     socket\u001b[38;5;241m.\u001b[39mgethostname() \u001b[38;5;241m+\u001b[39m filename_suffix\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outstanding_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_py_recordio_writer \u001b[38;5;241m=\u001b[39m \u001b[43mRecordWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Initialize an event instance.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event \u001b[38;5;241m=\u001b[39m event_pb2\u001b[38;5;241m.\u001b[39mEvent()\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\tensorboardX\\record_writer.py:182\u001b[0m, in \u001b[0;36mRecordWriter.__init__\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer \u001b[38;5;241m=\u001b[39m \u001b[43mopen_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\UGRP\\lib\\site-packages\\tensorboardX\\record_writer.py:61\u001b[0m, in \u001b[0;36mopen_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m factory\u001b[38;5;241m.\u001b[39mopen(path)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/stell/AppData/Local/Temp/ray/session_2024-09-11_21-58-26_140596_26880/artifacts/2024-09-11_22-02-56/train_ray_2024-09-11_22-02-56/driver_artifacts/train_ray_2a16c_00000_0_batch_size=2,l1=512,lr=0.0001_2024-09-11_22-02-57\\\\events.out.tfevents.1726059777.DESKTOP-T9EUN1U'"
     ]
    }
   ],
   "source": [
    "def main(num_samples=32, max_num_epochs=10, gpus_per_trial=1):\n",
    "\n",
    "    datapath=  \"C:/Users/stell/OneDrive/바탕 화면/10000_data/\"\n",
    "    load_data(datapath)\n",
    "    config = {\n",
    "        \"l1\": tune.choice([2**i for i in range(7, 12)]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([2, 4, 8, 16]),\n",
    "    }\n",
    "    \n",
    "    print(\"pass1\")\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    print(\"pass2\")\n",
    "\n",
    "    result = tune.run(\n",
    "        partial(train_ray, data_dir=datapath),\n",
    "        resources_per_trial={\"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        #local_dir=\"./ray_results\"  # 체크포인트가 저장될 디렉토리 경로\n",
    "    )\n",
    "\n",
    "    print(\"pass3\")\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
    "\n",
    "    best_trained_model = CNN_MRA_1D(best_trial.config[\"l1\"])\n",
    "    # device = \"cpu\"\n",
    "    # if torch.cuda.is_available():\n",
    "    #     device = \"cuda:0\"\n",
    "    #     if gpus_per_trial > 1:\n",
    "    #         best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    print(\"pass4\")\n",
    "\n",
    "    best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric=\"loss\", mode=\"max\")\n",
    "    with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "        data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "        with open(data_path, \"rb\") as fp:\n",
    "            best_checkpoint_data = pickle.load(fp)\n",
    "\n",
    "        best_trained_model.load_state_dict(best_checkpoint_data[\"model_state_dict\"])\n",
    "        test_acc = test_accuracy(best_trained_model, device)\n",
    "        print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 매 실험당 사용할 GPU 수를 여기에서 변경할 수 있습니다:\n",
    "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 파라미터 저장\n",
    "# torch.save(model.state_dict(), '../../../saved_data/saved_model/CNN_IO_MRA_1D_swt_10000.pth')\n",
    "# torch.save(model.state_dict(), '../../../evaluation/CNN_IO_MRA_1D_swt_10000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 평가\n",
    "# Contaminated = torch.tensor([])\n",
    "# Clean = torch.tensor([])\n",
    "# SACed = torch.tensor([])\n",
    "\n",
    "# model.eval()\n",
    "# test_loss = 0.0\n",
    "# with torch.no_grad():\n",
    "#     for x, y in test_loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "        \n",
    "#         y_pred = model(x)\n",
    "#         loss = criterion(y_pred, y)\n",
    "#         test_loss += loss.item() * x.size(0)\n",
    "\n",
    "#         Contaminated = torch.cat((Contaminated, x.squeeze().cpu()), 0)\n",
    "#         SACed = torch.cat((SACed, y_pred.squeeze().cpu()), 0)\n",
    "#         Clean = torch.cat((Clean, y.squeeze().cpu()), 0)\n",
    "\n",
    "# test_loss /= len(test_loader.dataset)\n",
    "# print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MRA Plot\n",
    "# SACed_ex = SACed[0].detach().cpu()\n",
    "# Clean_ex = Clean[0].detach().cpu()\n",
    "\n",
    "# vmin, vmax = min(SACed_ex.min(), Clean_ex.min()), max(SACed_ex.max(), Clean_ex.max())\n",
    "\n",
    "# plt.figure(figsize=(20, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(SACed_ex, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax, extent=[t[0], t[-1], 11, 0])\n",
    "# plt.colorbar().set_label('Coefficient Value', fontsize=15)\n",
    "# plt.xlabel('time(s)', fontsize=15)\n",
    "# plt.ylabel('level', fontsize=15)\n",
    "# plt.title('SACed signal', fontsize=20)\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(Clean_ex, aspect='auto', cmap='viridis', vmin=vmin, vmax=vmax, extent=[t[0], t[-1], 11, 0])\n",
    "# plt.colorbar().set_label('Coefficient Value', fontsize=15)\n",
    "# plt.xlabel('time(s)', fontsize=15)\n",
    "# plt.ylabel('level', fontsize=15)\n",
    "# plt.title('Clean signal', fontsize=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# print(f\"Mean Absolute Error: {mean_absolute_error(Clean_ex, SACed_ex)}\")\n",
    "# print(f\"Mean Squared Error: {mean_squared_error(Clean_ex, SACed_ex)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MRA 결과 inverse scaling\n",
    "# Contaminated_inverse_scaled = []\n",
    "# SACed_inverse_scaled = []\n",
    "# Clean_inverse_scaled = []\n",
    "\n",
    "# for xx, yhat, yy in zip(Contaminated, SACed, Clean):\n",
    "#     flat_x = xx.flatten().reshape(-1, 1)\n",
    "#     flat_yhat = yhat.flatten().reshape(-1, 1)\n",
    "#     flat_y = yy.flatten().reshape(-1, 1)\n",
    "\n",
    "#     inv_flat_x = scaler_x.inverse_transform(flat_x)\n",
    "#     inv_flat_yhat = scaler_y.inverse_transform(flat_yhat)\n",
    "#     inv_flat_y = scaler_y.inverse_transform(flat_y)\n",
    "\n",
    "#     Contaminated_inverse_scaled.append(inv_flat_x.reshape(xx.shape))\n",
    "#     SACed_inverse_scaled.append(inv_flat_yhat.reshape(yhat.shape))\n",
    "#     Clean_inverse_scaled.append(inv_flat_y.reshape(yy.shape))\n",
    "\n",
    "# Contaminated_inverse_scaled = np.array(Contaminated_inverse_scaled)\n",
    "# SACed_inverse_scaled = np.array(SACed_inverse_scaled)\n",
    "# Clean_inverse_scaled = np.array(Clean_inverse_scaled)\n",
    "\n",
    "\n",
    "# # MRA 데이터를 inverse하여 시계열 데이터로 변환\n",
    "\n",
    "# Contaminated_invdwt = []\n",
    "# SACed_invdwt = []\n",
    "# Clean_invdwt = []\n",
    "\n",
    "# for mra_x, mra_yhat, mra_y in zip(Contaminated_inverse_scaled, SACed_inverse_scaled, Clean_inverse_scaled):\n",
    "#     contaminated_invdwt = pywt.imra(mra_x)\n",
    "#     saced_invdwt = pywt.imra(mra_yhat)\n",
    "#     clean_invdwt = pywt.imra(mra_y)\n",
    "    \n",
    "#     Contaminated_invdwt.append(contaminated_invdwt)\n",
    "#     SACed_invdwt.append(saced_invdwt)\n",
    "#     Clean_invdwt.append(clean_invdwt)\n",
    "\n",
    "# Contaminated_invdwt = np.array(Contaminated_invdwt)\n",
    "# SACed_invdwt = np.array(SACed_invdwt)\n",
    "# Clean_invdwt = np.array(Clean_invdwt)\n",
    "\n",
    "# print(\"X_reconstructed:\", Contaminated_invdwt.shape)\n",
    "# print(\"yhat_reconstructed:\", SACed_invdwt.shape)\n",
    "# print(\"y_reconstructed:\", Clean_invdwt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 결과 Plot\n",
    "\n",
    "# save_path = '../../../result/data_10000/CNN/'\n",
    "# save_title = 'CNN_IO_MRA_1D_swt'\n",
    "\n",
    "# Result_Plot(Contaminated_invdwt, SACed_invdwt, Clean_invdwt, save_path, save_title) # inverse scaled data를 input으로 넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Paper 결과 저장\n",
    "# save_path = '../../../result/evaluation/'\n",
    "# save_title = 'CNN_IO_MRA_1D_swt_10000'\n",
    "# Result_Plot_paper(Contaminated_invdwt, SACed_invdwt, Clean_invdwt, save_path, save_title) # inverse scaled data를 input으로 넣음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
