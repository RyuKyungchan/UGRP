{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import welch\n",
    "\n",
    "import os\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Tuner\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import tempfile\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../tool_code/function/') # \"~~/tool_code/plot/\" (상대 경로)\n",
    "\n",
    "from DataPlot import Data_Load_Plot, Result_Plot, Train_Loss_Plot\n",
    "from Scaling import time_scaling, time_inv_scaling\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1000, 4000)\n",
      "y: (1000, 4000)\n",
      "(1000, 4000)\n",
      "(1000, 4000)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 load & plot\n",
    "fpath = \"../../data/synthetic_data/\"\n",
    "\n",
    "Contaminated_data = np.load(fpath + \"contaminated_by_realistic\" + \".npy\")\n",
    "Clean_data = np.load(fpath + \"clean_data\" + \".npy\")\n",
    "Artifact_daata = Contaminated_data - Clean_data\n",
    "\n",
    "# Data Standard Scaling\n",
    "X, y, scaler_x, scaler_y = time_scaling(Contaminated_data, Clean_data, standard='x')\n",
    "\n",
    "print(Contaminated_data.shape)\n",
    "print(Clean_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Original>\n",
      "-----------------------------\n",
      "X_train shape: (800, 4000)\n",
      "y_train shape: (800, 4000)\n",
      "-----------------------------\n",
      "X_test shape: (200, 4000)\n",
      "y_test shape: (200, 4000)\n",
      "-----------------------------\n",
      "<Unsqueezed>\n",
      "-----------------------------\n",
      "X_train shape: (800, 1, 4000)\n",
      "y_train shape: (800, 1, 4000)\n",
      "-----------------------------\n",
      "X_test shape: (200, 1, 4000)\n",
      "y_test shape: (200, 1, 4000)\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"<Original>\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_train shape: {X_train.shape}\\ny_train shape: {y_train.shape}\") # x : B x T, y : B x T\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_test shape: {X_test.shape}\\ny_test shape: {y_test.shape}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# 차원 추가 (LSTM은 세번째 차원 추가)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1]) # Batch x length x 1\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "y_train = y_train.reshape(y_train.shape[0], 1, y_train.shape[1]) # Batch x length x 1\n",
    "y_test = y_test.reshape(y_test.shape[0], 1, y_test.shape[1])\n",
    "\n",
    "print(\"<Unsqueezed>\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_train shape: {X_train.shape}\\ny_train shape: {y_train.shape}\") # x : B x T x 1 , y : B x T\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_test shape: {X_test.shape}\\ny_test shape: {y_test.shape}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "#train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "#test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "#train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "#test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): GELU(approximate='none')\n",
      "    (5): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (6): GELU(approximate='none')\n",
      "    (7): Conv1d(16, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (drop): Dropout1d(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, hidden_dim=16, kernel_size=3, dropout_rate=0.25):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, hidden_dim, kernel_size, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size, padding=1),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(hidden_dim, out_channels, kernel_size, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.drop = nn.Dropout1d(dropout_rate)\n",
    "\n",
    "    def forward(self, x):  # x: B x 1 x T\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "\n",
    "model = CNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 06:57:27,629\tINFO worker.py:1781 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "os.environ[\"RAY_TMPDIR\"] = \"C:/ray_tmp\"\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "def train_cnn(config):\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    model = CNN(config[\"hidden_dim\"], config[\"kernel_size\"], config[\"dropout_rate\"]).to(config[\"device\"])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(config[\"device\"]), labels.to(config[\"device\"])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # 평균 손실값을 기록\n",
    "        tune.report(loss=running_loss / len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 206] 파일 이름이나 확장명이 너무 깁니다: 'C:\\\\ray_tmp\\\\ray\\\\session_2024-08-19_06-57-25_749321_11860\\\\artifacts\\\\2024-08-19_06-57-31\\\\train_cnn_2024-08-19_06-57-31\\\\driver_artifacts\\\\train_cnn_de2cc_00000_0_batch_size=16,dropout_rate=0.1793,hidden_dim=16,kernel_size=5,lr=0.0025_2024-08-19_07-00-10'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\pathlib.py:1323\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[1;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 206] 파일 이름이나 확장명이 너무 깁니다: 'C:\\\\ray_tmp\\\\ray\\\\session_2024-08-19_06-57-25_749321_11860\\\\artifacts\\\\2024-08-19_06-57-31\\\\train_cnn_2024-08-19_06-57-31\\\\driver_artifacts\\\\train_cnn_de2cc_00000_0_batch_size=16,dropout_rate=0.1793,hidden_dim=16,kernel_size=5,lr=0.0025_2024-08-19_07-00-10'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 실행\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 34\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[0;32m     15\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ASHAScheduler(\n\u001b[0;32m     16\u001b[0m     max_t\u001b[38;5;241m=\u001b[39mmax_num_epochs,\n\u001b[0;32m     17\u001b[0m     grace_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     18\u001b[0m     reduction_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m tuner \u001b[38;5;241m=\u001b[39m Tuner(\n\u001b[0;32m     22\u001b[0m     tune\u001b[38;5;241m.\u001b[39mwith_resources(\n\u001b[0;32m     23\u001b[0m         tune\u001b[38;5;241m.\u001b[39mwith_parameters(train_cnn),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     param_space\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m best_result \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mget_best_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_result\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\site-packages\\ray\\tune\\tuner.py:377\u001b[0m, in \u001b[0;36mTuner.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Executes hyperparameter tuning job as configured and returns result.\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03mFailure handling:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    RayTaskError: If user-provided trainable raises an exception\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_ray_client:\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    379\u001b[0m     (\n\u001b[0;32m    380\u001b[0m         progress_reporter,\n\u001b[0;32m    381\u001b[0m         string_queue,\n\u001b[0;32m    382\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_remote_tuner_for_jupyter_progress_reporting()\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py:476\u001b[0m, in \u001b[0;36mTunerInternal.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    474\u001b[0m param_space \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_space)\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_restored:\n\u001b[1;32m--> 476\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resume(trainable, param_space)\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\site-packages\\ray\\tune\\impl\\tuner_internal.py:592\u001b[0m, in \u001b[0;36mTunerInternal._fit_internal\u001b[1;34m(self, trainable, param_space)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fitting for a fresh Tuner.\"\"\"\u001b[39;00m\n\u001b[0;32m    580\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tune_run_arguments(trainable),\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuner_kwargs,\n\u001b[0;32m    591\u001b[0m }\n\u001b[1;32m--> 592\u001b[0m analysis \u001b[38;5;241m=\u001b[39m run(\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    594\u001b[0m )\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_remote_string_queue()\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\site-packages\\ray\\tune\\tune.py:994\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mis_finished() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m experiment_interrupted_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m--> 994\u001b[0m         \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m has_verbosity(Verbosity\u001b[38;5;241m.\u001b[39mV1_EXPERIMENT):\n\u001b[0;32m    996\u001b[0m             _report_progress(runner, progress_reporter)\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:682\u001b[0m, in \u001b[0;36mTuneController.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_update_trial_queue()\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# Start actors for added trials\u001b[39;00m\n\u001b[1;32m--> 682\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_add_actors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;66;03m# Handle one event\u001b[39;00m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_manager\u001b[38;5;241m.\u001b[39mnext(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;66;03m# If there are no actors running, warn about potentially\u001b[39;00m\n\u001b[0;32m    687\u001b[0m     \u001b[38;5;66;03m# insufficient resources\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:860\u001b[0m, in \u001b[0;36mTuneController._maybe_add_actors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actor_cache\u001b[38;5;241m.\u001b[39mincrease_max(trial_to_run\u001b[38;5;241m.\u001b[39mplacement_group_factory)\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# schedule_trial_actor also potentially uses cached actors\u001b[39;00m\n\u001b[1;32m--> 860\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule_trial_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_to_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;66;03m# Otherwise, only try to use the cached actor\u001b[39;00m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _dedup_logs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrial_to_run_reuse\u001b[39m\u001b[38;5;124m\"\u001b[39m, trial_to_run\u001b[38;5;241m.\u001b[39mtrial_id):\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\site-packages\\ray\\tune\\execution\\tune_controller.py:974\u001b[0m, in \u001b[0;36mTuneController._schedule_trial_actor\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    970\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to schedule new ACTOR for trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m Trial\u001b[38;5;241m.\u001b[39mPENDING\n\u001b[1;32m--> 974\u001b[0m \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_local_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;66;03m# We checkpoint metadata here to try mitigating logdir duplication\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_trial_to_checkpoint(trial)\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\site-packages\\ray\\tune\\experiment\\trial.py:653\u001b[0m, in \u001b[0;36mTrial.init_local_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(logdir_path)) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_path_length:\n\u001b[0;32m    647\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe path to the trial log directory is too long \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    649\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(max length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_path_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using `trial_dirname_creator` to shorten the path. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogdir_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m     )\n\u001b[1;32m--> 653\u001b[0m \u001b[43mlogdir_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvalidate_json_state()\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\pathlib.py:1328\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[1;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[0;32m   1326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1328\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_dir():\n",
      "File \u001b[1;32mc:\\Users\\stell\\anaconda3\\envs\\torch1\\lib\\pathlib.py:1323\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[1;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;124;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 206] 파일 이름이나 확장명이 너무 깁니다: 'C:\\\\ray_tmp\\\\ray\\\\session_2024-08-19_06-57-25_749321_11860\\\\artifacts\\\\2024-08-19_06-57-31\\\\train_cnn_2024-08-19_06-57-31\\\\driver_artifacts\\\\train_cnn_de2cc_00000_0_batch_size=16,dropout_rate=0.1793,hidden_dim=16,kernel_size=5,lr=0.0025_2024-08-19_07-00-10'"
     ]
    }
   ],
   "source": [
    "# 임시 디렉토리 경로 변경\n",
    "\n",
    "def main(num_samples=1000, max_num_epochs=10, gpus_per_trial=0):\n",
    "    config = {\n",
    "        \"hidden_dim\": tune.choice([16, 32, 64]),\n",
    "        \"kernel_size\": tune.choice([3, 5, 7]),\n",
    "        \"dropout_rate\": tune.uniform(0.1, 0.5),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "        \"batch_size\": tune.choice([16, 32, 64]),\n",
    "        \"epochs\": max_num_epochs,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "    \n",
    "    tuner = Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_cnn),\n",
    "            resources={\"cpu\": 2, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"loss\", \"min\", \"last\")\n",
    "\n",
    "    print(f\"Best trial config: {best_result.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_result.metrics['loss']}\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    main(num_samples=1000, max_num_epochs=10, gpus_per_trial=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
