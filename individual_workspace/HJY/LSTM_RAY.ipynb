{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import welch\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../tool_code/function/')\n",
    "\n",
    "from DataPlot import Data_Load_Plot, Result_Plot, Train_Loss_Plot\n",
    "from Scaling import time_scaling, time_inv_scaling\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4000)\n",
      "(1000, 4000)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 load & plot\n",
    "fpath = \"../../data/synthetic_data/\"\n",
    "\n",
    "Contaminated_data = np.load(fpath + \"contaminated_by_realistic\" + \".npy\")\n",
    "Clean_data = np.load(fpath + \"clean_data\" + \".npy\")\n",
    "Artifact_daata = Contaminated_data - Clean_data\n",
    "\n",
    "print(Contaminated_data.shape)\n",
    "print(Clean_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1000, 4000)\n",
      "y: (1000, 4000)\n"
     ]
    }
   ],
   "source": [
    "# Data Standard Scaling\n",
    "X, y, scaler_x, scaler_y = time_scaling(Contaminated_data, Clean_data, standard='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Original>\n",
      "-----------------------------\n",
      "X_train shape: (800, 4000)\n",
      "y_train shape: (800, 4000)\n",
      "-----------------------------\n",
      "X_test shape: (200, 4000)\n",
      "y_test shape: (200, 4000)\n",
      "-----------------------------\n",
      "<Unsqueezed>\n",
      "-----------------------------\n",
      "X_train shape: (800, 4000, 1)\n",
      "y_train shape: (800, 4000, 1)\n",
      "-----------------------------\n",
      "X_test shape: (200, 4000, 1)\n",
      "y_test shape: (200, 4000, 1)\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"<Original>\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_train shape: {X_train.shape}\\ny_train shape: {y_train.shape}\") # x : B x T, y : B x T\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_test shape: {X_test.shape}\\ny_test shape: {y_test.shape}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "# 차원 추가 (LSTM은 세번째 차원 추가)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1) # Batch x length x 1\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "y_train = y_train.reshape(y_train.shape[0], y_train.shape[1], 1) # Batch x length x 1\n",
    "y_test = y_test.reshape(y_test.shape[0], y_test.shape[1], 1)\n",
    "\n",
    "print(\"<Unsqueezed>\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_train shape: {X_train.shape}\\ny_train shape: {y_train.shape}\") # x : B x T x 1 , y : B x T\n",
    "print(\"-----------------------------\")\n",
    "print(f\"X_test shape: {X_test.shape}\\ny_test shape: {y_test.shape}\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Block(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_Block, self).__init__()\n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size,\n",
    "                                num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) # 은닉 상태를 0으로 초기화\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) # 셀 상태를 0으로 초기화\n",
    "    \n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # LSTM 계층에 은닉 상태와 셀 상태 적용\n",
    "        # output = output.reshape(-1, self.hidden_size) # fc layer 적용을 위해 데이터를 1차원 형태로 조정\n",
    "        out = self.gelu(output)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class LSTM_time(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.net = nn.Sequential(*[\n",
    "            LSTM_Block(hidden_size, hidden_size, 1)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x): # B x T x 1\n",
    "        x = self.gelu(self.dropout(self.fc1(x))) # x: B x T x 128\n",
    "        x = self.net(x) # x: B x T x 128\n",
    "        x = self.fc2(x) # x: B x T x 1 -> B x T\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_time(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (relu): ReLU()\n",
      "  (fc1): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (net): Sequential(\n",
      "    (0): LSTM_Block(\n",
      "      (lstm): LSTM(128, 128, batch_first=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "    (1): LSTM_Block(\n",
      "      (lstm): LSTM(128, 128, batch_first=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스 생성\n",
    "input_size = 1  # 입력 크기\n",
    "hidden_size = 128 # 임의의 hidden layer 크기\n",
    "output_size = 1  # 출력 크기\n",
    "num_layers = 2  # 임의의 LSTM layer 개수\n",
    "\n",
    "model = LSTM_time(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "print(model)\n",
    "    \n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0028891759365797043\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "Contaminated = torch.tensor([])\n",
    "Clean = torch.tensor([])\n",
    "SACed = torch.tensor([])\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        test_loss += loss.item() * x.size(0)\n",
    "\n",
    "        Contaminated = torch.cat((Contaminated, x.squeeze().cpu()), 0)\n",
    "        SACed = torch.cat((SACed, y_pred.squeeze().cpu()), 0)\n",
    "        Clean = torch.cat((Clean, y.squeeze().cpu()), 0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습함수 정의\n",
    "\n",
    "def train_lstm(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LSTM_time(input_size = 1, \n",
    "                      hidden_size = config[\"hidden_size\"], \n",
    "                      output_size = 1, \n",
    "                      num_layers = config[\"num_layers\"]).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            loss_list.append(epoch_loss)\n",
    "        if (epoch+1)%1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] | Loss: {epoch_loss}')\n",
    "\n",
    "        Contaminated = torch.tensor([])\n",
    "        Clean = torch.tensor([])\n",
    "        SACed = torch.tensor([])\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                \n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                test_loss += loss.item() * x.size(0)\n",
    "\n",
    "                Contaminated = torch.cat((Contaminated, x.squeeze().cpu()), 0)\n",
    "                SACed = torch.cat((SACed, y_pred.squeeze().cpu()), 0)\n",
    "                Clean = torch.cat((Clean, y.squeeze().cpu()), 0)\n",
    "\n",
    "        train.report({\"loss\": running_loss / len(train_loader.dataset) })    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-08-10 10:36:05</td></tr>\n",
       "<tr><td>Running for: </td><td>03:20:47.24        </td></tr>\n",
       "<tr><td>Memory:      </td><td>15.8/39.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=1<br>Bracket: Iter 512.000: -6.505528188426978e-05 | Iter 256.000: -7.956943707540631e-05 | Iter 128.000: -9.817408979870379e-05 | Iter 64.000: -0.00015926491119898855 | Iter 32.000: -0.0029084241669625043 | Iter 16.000: -0.002915016049519181 | Iter 8.000: -0.0029175811354070903 | Iter 4.000: -0.002919740956276655 | Iter 2.000: -0.003154013054445386 | Iter 1.000: -0.03503108199685812<br>Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:TITAN)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">       loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_lstm_dc052_00000</td><td>TERMINATED</td><td>127.0.0.1:15348</td><td style=\"text-align: right;\">0.00658187</td><td style=\"text-align: right;\">  1000</td><td style=\"text-align: right;\">           12040</td><td style=\"text-align: right;\">6.59639e-05</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 10:36:05,300\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/Users/stell/ray_results/train_lstm_2024-08-10_07-15-17' in 0.0083s.\n",
      "2024-08-10 10:36:05,308\tINFO tune.py:1041 -- Total run time: 12047.72 seconds (12047.23 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'lr': 0.006581873080791374, 'epochs': 1000, 'num_layers': 7, 'hidden_size': 100}\n",
      "Best trial final validation loss: 6.596390827326104e-05\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# Set this to True for a smoke test that runs with a small synthetic dataset.\n",
    "SMOKE_TEST = False\n",
    "\n",
    "def main(num_samples, max_num_epochs, gpus_per_trial=2, smoke_test=False):\n",
    "    config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"epochs\": max_num_epochs,\n",
    "    \"num_layers\": tune.sample_from(lambda _: np.random.randint(1, 20)),\n",
    "    \"hidden_size\" : tune.sample_from(lambda _: np.random.randint(100, 200))\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2\n",
    "    )\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_lstm),\n",
    "            resources={\"cpu\": 2, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",\n",
    "            mode=\"min\",\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"loss\", \"min\", \"last\")\n",
    "\n",
    "    print(f\"Best trial config: {best_result.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_result.metrics['loss']}\")\n",
    "    \n",
    "    #test_best_model(best_result, smoke_test=smoke_test)\n",
    "\n",
    "main(num_samples=1, max_num_epochs=3000, gpus_per_trial=0, smoke_test=SMOKE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
